{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run MASK-RCNN on all of my real RGBD Images from TableTop_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "import open3d\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes our figures bigger\n",
    "pylab.rcParams['figure.figsize'] = 20, 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the relevant imports for the detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_rgb = True\n",
    "use_depth = True\n",
    "use_pretrained = False\n",
    "# Must be a combo of: [rgb, depth, rgb+depth, pretrained]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maskrcnn_benchmark.config import cfg # Restart kernel everytime you want to load a new config...\n",
    "import predictor\n",
    "predictor = reload(predictor)\n",
    "from maskrcnn_benchmark.data.datasets.tabletop_object_dataset import compute_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maskrcnn_benchmark.data.datasets.tabletop_object_dataset import data_loading_params\n",
    "import maskrcnn_benchmark.data.datasets.data_augmentation as data_augmentation\n",
    "import maskrcnn_benchmark.data.datasets.util as util_\n",
    "\n",
    "if use_rgb and not use_depth:\n",
    "    config_file = \"../configs/e2e_mask_rcnn_R_50_FPN_1x_TTOD_RGB.yaml\"\n",
    "elif not use_rgb and use_depth:\n",
    "    config_file = \"../configs/e2e_mask_rcnn_R_50_FPN_1x_TTOD_Depth.yaml\"\n",
    "elif use_rgb and use_depth:\n",
    "    config_file = \"../configs/e2e_mask_rcnn_R_50_FPN_1x_TTOD_RGBD.yaml\"\n",
    "elif use_pretrained:\n",
    "    config_file = \"../configs/caffe2/e2e_mask_rcnn_X-152-32x8d-FPN-IN5k_1.44x_caffe2.yaml\"\n",
    "    \n",
    "cfg.merge_from_file(config_file)\n",
    "cfg['INPUT']['USE_RGB'] = use_rgb\n",
    "cfg['INPUT']['USE_DEPTH'] = use_depth\n",
    "if use_rgb or use_depth:\n",
    "    demo = predictor.Tabletop_Object_Demo(\n",
    "        cfg,\n",
    "        confidence_threshold=0.7,\n",
    "        show_mask_map_nice=False,\n",
    "        show_mask_map_raw=True,    \n",
    "    )\n",
    "elif use_pretrained:\n",
    "    demo = predictor.COCODemo(\n",
    "        cfg,\n",
    "        min_image_size=800,\n",
    "        confidence_threshold=0.7,\n",
    "        show_mask_map_nice=False,\n",
    "        show_mask_map_raw=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # Plot a BGR image\n",
    "    plt.imshow(img[:, :, [2, 1, 0]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the predictions\n",
    "\n",
    "We provide a `run_on_opencv_image` function, which takes an image as it was loaded by OpenCV (in `BGR` format), and computes the predictions on them, returning an image with the predictions overlayed on the image.\n",
    "\n",
    "We save the outputs to file.\n",
    "\n",
    "Note:\n",
    "* Input for `demo.run_on_opencv_image()` is a BGR uint8 image.\n",
    "* Output of `demo.run_on_opencv_image()` is a BGR uint8 image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_camera_params = {\n",
    "    'img_width' : 640, \n",
    "    'img_height' : 480,\n",
    "    'near' : 0.01,\n",
    "    'far' : 100,\n",
    "    'fov' : 60, # vertical field of view in angles\n",
    "}\n",
    "\n",
    "scene_num = 58\n",
    "view_num = 3\n",
    "synth_img_filename = f'scene_{scene_num:05d}/' + \\\n",
    "                     ('rgb' if use_rgb else 'depth') + \\\n",
    "                     f'_{view_num:05d}.' + \\\n",
    "                     ('jpeg' if use_rgb else 'png')\n",
    "if use_rgb:\n",
    "    # img_filename = '/data/tabletop_dataset_v2/real_RGBD_images/rgb_00002.jpeg'\n",
    "    img_filename = '/data/tabletop_dataset_v3/test_set/' + synth_img_filename\n",
    "    img = cv2.imread(img_filename)\n",
    "    rgb_image = img.copy()\n",
    "elif use_depth:\n",
    "    img_filename = '/data/tabletop_dataset_v3/test_set/' + synth_img_filename\n",
    "    img = cv2.imread(img_filename, cv2.IMREAD_ANYDEPTH)\n",
    "    img = (img / 1000.).astype(np.float32)\n",
    "    img = compute_xyz(img, synth_camera_params)\n",
    "    rgb_filename = img_filename.replace('depth', 'rgb').replace('png', 'jpeg')\n",
    "    rgb_image = cv2.imread(rgb_filename)    \n",
    "    \n",
    "predictions = demo.run_on_opencv_image(img, rgb_image) # BGR format\n",
    "imshow(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All predictions on Real RGBD Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_rgb:\n",
    "    rgb_images = sorted(glob.glob('/data/tabletop_dataset_v5/real_RGBD_images/rgb*'))\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/maskrcnn/detections/tabletop_object_train_RGB/'\n",
    "    N = len(rgb_images)\n",
    "    \n",
    "if use_depth:\n",
    "    depth_images = sorted(glob.glob('/data/tabletop_dataset_v5/real_RGBD_images/depth*'))\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/maskrcnn/detections/tabletop_object_train_Depth/'\n",
    "    \n",
    "    # Get camera parameters\n",
    "    camera_params_filename = '/data/tabletop_dataset_v5/real_RGBD_images/camera_params.json'\n",
    "    camera_params_dict = json.load(open(camera_params_filename))\n",
    "    \n",
    "    N = len(depth_images)\n",
    "\n",
    "if use_pretrained:\n",
    "    rgb_images = sorted(glob.glob('/data/tabletop_dataset_v5/real_RGBD_images/rgb*'))\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/maskrcnn/detections/coco_train/'\n",
    "    N = len(rgb_images)\n",
    "    \n",
    "if use_rgb and use_depth:\n",
    "    save_dir = '/home/chrisxie/temp/'\n",
    "    N = len(rgb_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):\n",
    "    if (use_rgb and not use_depth):\n",
    "        img = cv2.imread(rgb_images[i])\n",
    "        rgb_img = img.copy()\n",
    "        \n",
    "        # Preprocess\n",
    "        rgb_img_tensor = rgb_img.astype(np.float32)\n",
    "        rgb_img_tensor = data_augmentation.BGR_image(rgb_img_tensor)\n",
    "        img = data_augmentation.array_to_tensor(rgb_img_tensor)\n",
    "        \n",
    "    elif (not use_rgb and use_depth):\n",
    "        depth_img = cv2.imread(depth_images[i], cv2.IMREAD_ANYDEPTH)\n",
    "        depth_img = (depth_img / 1000.).astype(np.float32)\n",
    "        depth_img = compute_xyz(depth_img, camera_params_dict) # Shape: [H x W x 3], dtype=np.float32\n",
    "        base_dir = '/'.join(image.split('/')[:-1]) + '/'\n",
    "        rgb_filename = depth_images[i].split('/')[-1].replace('depth', 'rgb').replace('png', 'jpeg')\n",
    "        rgb_filename = base_dir + rgb_filename\n",
    "        rgb_img = cv2.imread(rgb_filename)\n",
    "        \n",
    "        # Preprocess\n",
    "        img = data_augmentation.array_to_tensor(xyz_img)\n",
    "        \n",
    "    elif use_rgb and use_depth:\n",
    "        rgb_img = cv2.imread(rgb_images[i])\n",
    "        depth_img = cv2.imread(depth_images[i], cv2.IMREAD_ANYDEPTH)\n",
    "        depth_img = (depth_img / 1000.).astype(np.float32)\n",
    "        xyz_img = compute_xyz(depth_img, camera_params_dict) # Shape: [H x W x 3], dtype=np.float32\n",
    "        \n",
    "        # Preprocess\n",
    "        rgb_img_tensor = rgb_img.astype(np.float32)\n",
    "        rgb_img_tensor = data_augmentation.BGR_image(rgb_img_tensor)\n",
    "        rgb_img_tensor = data_augmentation.array_to_tensor(rgb_img_tensor)\n",
    "        xyz_img = data_augmentation.array_to_tensor(xyz_img)\n",
    "        img = torch.cat([rgb_img_tensor, xyz_img], dim=0)\n",
    "        \n",
    "    elif use_pretrained:\n",
    "        img = cv2.imread(image)\n",
    "        rgb_image = img.copy()\n",
    "        \n",
    "    predictions = demo.run_on_opencv_image(img, rgb_img) # BGR format for both input and output.\n",
    "    \n",
    "    save_filename = save_dir + f'maskrcnn_{i}.png'\n",
    "    cv2.imwrite(save_filename, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on TODv5 Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_rgb and not use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/TODv5_results/test_set/Mask_RCNN/RGB/'\n",
    "elif not use_rgb and use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/TODv5_results/test_set/Mask_RCNN/Depth/'\n",
    "elif use_rgb and use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/TODv5_results/test_set/Mask_RCNN/RGBD/'\n",
    "elif use_pretrained:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/TODv5_results/test_set/Mask_RCNN/Pretrained/'\n",
    "    \n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maskrcnn_benchmark.data.datasets.tabletop_object_dataset as TOD\n",
    "TOD = reload(TOD)\n",
    "TOD_test_filepath = '/data/tabletop_dataset_v5/test_set/'\n",
    "dl = TOD.TOD_test_dataloader(TOD_test_filepath, batch_size=1, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "progress = tqdm(dl)\n",
    "for batch in progress:\n",
    "    \n",
    "    rgb_img = batch['rgb'][0]  # [3, H, W]\n",
    "    xyz_img = batch['xyz'][0]  # [3, H, W]\n",
    "    \n",
    "    # Create input to Mask RCNN. Preprocessing is done here (exactly like in maskrcnn_benchmark.data.datasets.tabletop_object_dataset.py)\n",
    "    if (use_rgb and not use_depth):\n",
    "        img = rgb_img\n",
    "    elif (use_depth and not use_rgb):\n",
    "        img = xyz_img\n",
    "    elif (use_rgb and use_depth):\n",
    "        img = torch.cat([rgb_img, xyz_img], dim=0)\n",
    "    elif use_pretrained:\n",
    "        img = rgb_img.copy()\n",
    "    \n",
    "    # Run model\n",
    "    rgb_img_np = rgb_img.permute(1,2,0).numpy()\n",
    "    rgb_img_np = cv2.cvtColor(rgb_img_np, cv2.COLOR_BGR2RGB)\n",
    "    predictions = demo.run_on_opencv_image(img, rgb_img_np) # if show_mask_maps_raw=True, this should be shape: [H x W]\n",
    "\n",
    "    # Write results to disk\n",
    "    file_path = save_dir + batch['label_abs_path'][0].rsplit('/', 1)[0] + '/'\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    file_name = file_path + batch['label_abs_path'][0].rsplit('/', 1)[1].rsplit('.', 1)[0] + '.png'\n",
    "    util_.imwrite_indexed(file_name, predictions.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on OCID images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_rgb and not use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/OCID_results/Mask_RCNN/RGB/'\n",
    "elif not use_rgb and use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/OCID_results/Mask_RCNN/Depth/'\n",
    "elif use_rgb and use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/OCID_results/Mask_RCNN/RGBD/'\n",
    "elif use_pretrained:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/OCID_results/Mask_RCNN/Pretrained/'\n",
    "    \n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "f = open('/data/OCID-dataset/pcd_files.txt', 'r')\n",
    "pcd_files = [x.strip() for x in f.readlines()]\n",
    "# pcd_files = pcd_files[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for pcd_filename in tqdm(pcd_files):\n",
    "#     print(pcd_filename)\n",
    "\n",
    "    ### Process .pcd file ###\n",
    "    temp_idx = pcd_filename.split('/').index('OCID-dataset') # parse something like this: /data/OCID-dataset/YCB10/table/top/curved/seq36/pcd/result_2018-08-24-15-13-13.pcd\n",
    "    label_abs_path = '/'.join(pcd_filename.split('/')[temp_idx+1:])\n",
    "    point_cloud = open3d.read_point_cloud(pcd_filename)\n",
    "    \n",
    "    # Fill in missing pixel values for RGB\n",
    "    num_missing = 480*640 - np.asarray(point_cloud.colors).shape[0]\n",
    "    filled_in_rgb_img = np.concatenate([np.asarray(point_cloud.colors), np.zeros((num_missing,3))])\n",
    "    rgb_img = np.round(255 * filled_in_rgb_img.reshape(480,640,3)).astype(np.uint8)\n",
    "        \n",
    "    if use_depth:\n",
    "        # Fill in missing xyz values\n",
    "        num_missing = 480*640 - np.asarray(point_cloud.points).shape[0]\n",
    "        filled_in_points = np.concatenate([np.asarray(point_cloud.points), np.zeros((num_missing,3))])\n",
    "        xyz_img = np.asarray(filled_in_points).reshape(480,640,3)\n",
    "        xyz_img[np.isnan(xyz_img)] = 0\n",
    "        \n",
    "    # Create input to Mask RCNN. Preprocessing is done here (exactly like in maskrcnn_benchmark.data.datasets.tabletop_object_dataset.py)\n",
    "    if (use_rgb and not use_depth):\n",
    "        rgb_img_tensor = rgb_img.astype(np.float32)\n",
    "        rgb_img_tensor = data_augmentation.BGR_image(rgb_img_tensor)\n",
    "        img = data_augmentation.array_to_tensor(rgb_img_tensor)\n",
    "        \n",
    "    elif (use_depth and not use_rgb):\n",
    "        img = data_augmentation.array_to_tensor(xyz_img)\n",
    "        \n",
    "    elif (use_rgb and use_depth):\n",
    "        rgb_img_tensor = rgb_img.astype(np.float32)\n",
    "        rgb_img_tensor = data_augmentation.BGR_image(rgb_img_tensor)\n",
    "        rgb_img_tensor = data_augmentation.array_to_tensor(rgb_img_tensor)\n",
    "        xyz_img = data_augmentation.array_to_tensor(xyz_img)\n",
    "        img = torch.cat([rgb_img_tensor, xyz_img], dim=0)\n",
    "        \n",
    "    elif use_pretrained:\n",
    "        img = rgb_img.copy()\n",
    "    \n",
    "    \n",
    "    ### Run the thing ###\n",
    "    # Note: img is a pytorch Tensor, rgb_img is np.array of type np.uint8\n",
    "    predictions = demo.run_on_opencv_image(img, rgb_img) # if show_mask_maps_raw=True, this should be shape: [H x W]\n",
    "    \n",
    "    \n",
    "    ### Write out the results ###\n",
    "    file_path = save_dir + label_abs_path.rsplit('/', 1)[0] + '/'\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    file_name = file_path + label_abs_path.rsplit('/', 1)[1].rsplit('.', 1)[0] + '.png'\n",
    "    util_.imwrite_indexed(file_name, predictions.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on OSD images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_rgb and not use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/OSD_results/Mask_RCNN/RGB/'\n",
    "elif not use_rgb and use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/OSD_results/Mask_RCNN/Depth/'\n",
    "elif use_rgb and use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/OSD_results/Mask_RCNN/RGBD/'\n",
    "elif use_pretrained:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/OSD_results/Mask_RCNN/Pretrained/'\n",
    "    \n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "f = open('/data/OSD/pcd_files.txt', 'r')\n",
    "pcd_files = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for pcd_filename in tqdm(pcd_files):\n",
    "    \n",
    "#     print(pcd_filename)\n",
    "    \n",
    "    ### Process .pcd file ###\n",
    "    temp_idx = pcd_filename.split('/').index('OSD') # parse something like this: /data/OSD/OSD-0.2/pcd/learn44.pcd\n",
    "    label_abs_path = '/'.join(pcd_filename.split('/')[temp_idx+1:])\n",
    "    point_cloud = open3d.read_point_cloud(pcd_filename)\n",
    "    \n",
    "    # Fill in missing pixel values for RGB\n",
    "    num_missing = 480*640 - np.asarray(point_cloud.colors).shape[0]\n",
    "    filled_in_rgb_img = np.concatenate([np.asarray(point_cloud.colors), np.zeros((num_missing,3))])\n",
    "    rgb_img = np.round(255 * filled_in_rgb_img.reshape(480,640,3)).astype(np.uint8)\n",
    "        \n",
    "    if use_depth:\n",
    "        # Fill in missing xyz values\n",
    "        num_missing = 480*640 - np.asarray(point_cloud.points).shape[0]\n",
    "        filled_in_points = np.concatenate([np.asarray(point_cloud.points), np.zeros((num_missing,3))])\n",
    "        xyz_img = np.asarray(filled_in_points).reshape(480,640,3)\n",
    "        xyz_img[np.isnan(xyz_img)] = 0\n",
    "        \n",
    "    # Create input to Mask RCNN. Preprocessing is done here (exactly like in maskrcnn_benchmark.data.datasets.tabletop_object_dataset.py)\n",
    "    if (use_rgb and not use_depth):\n",
    "        rgb_img_tensor = rgb_img.astype(np.float32)\n",
    "        rgb_img_tensor = data_augmentation.BGR_image(rgb_img_tensor)\n",
    "        img = data_augmentation.array_to_tensor(rgb_img_tensor)\n",
    "        \n",
    "    elif (use_depth and not use_rgb):\n",
    "        img = data_augmentation.array_to_tensor(xyz_img)\n",
    "        \n",
    "    elif (use_rgb and use_depth):\n",
    "        rgb_img_tensor = rgb_img.astype(np.float32)\n",
    "        rgb_img_tensor = data_augmentation.BGR_image(rgb_img_tensor)\n",
    "        rgb_img_tensor = data_augmentation.array_to_tensor(rgb_img_tensor)\n",
    "        xyz_img = data_augmentation.array_to_tensor(xyz_img)\n",
    "        img = torch.cat([rgb_img_tensor, xyz_img], dim=0)\n",
    "        \n",
    "    elif use_pretrained:\n",
    "        img = rgb_img.copy()\n",
    "    \n",
    "    \n",
    "    ### Run the thing ###\n",
    "    # Note: img is a pytorch Tensor, rgb_img is np.array of type np.uint8\n",
    "    predictions = demo.run_on_opencv_image(img, rgb_img) # if show_mask_maps_raw=True, this should be shape: [H x W]\n",
    "    \n",
    "    \n",
    "    ### Write out the results ###\n",
    "    file_path = save_dir + label_abs_path.rsplit('/', 1)[0] + '/'\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    file_name = file_path + label_abs_path.rsplit('/', 1)[1].rsplit('.', 1)[0] + '.png'\n",
    "    util_.imwrite_indexed(file_name, predictions.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on ClearGrasp Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_rgb and not use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/cleargrasp_results/Mask_RCNN/RGB/'\n",
    "elif not use_rgb and use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/cleargrasp_results/Mask_RCNN/Depth/'\n",
    "elif use_rgb and use_depth:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/cleargrasp_results/Mask_RCNN/RGBD/'\n",
    "elif use_pretrained:\n",
    "    save_dir = '/home/chrisxie/projects/ssc/external/cleargrasp_results/Mask_RCNN/Pretrained/'\n",
    "    \n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model on entire dataset\n",
    "import maskrcnn_benchmark.data.datasets.cleargrasp_object as cg_dl\n",
    "cg_dl = reload(cg_dl)\n",
    "dl = cg_dl.get_CG_dataloader(batch_size=1, num_workers=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "progress = tqdm(dl)\n",
    "for batch in progress:\n",
    "    \n",
    "    rgb_img = batch['rgb'].numpy()[0]\n",
    "    xyz_img = batch['xyz'][0]\n",
    "        \n",
    "    # Create input to Mask RCNN. Preprocessing is done here (exactly like in maskrcnn_benchmark.data.datasets.tabletop_object_dataset.py)\n",
    "    if (use_rgb and not use_depth):\n",
    "        rgb_img_tensor = rgb_img.astype(np.float32)\n",
    "        rgb_img_tensor = data_augmentation.BGR_image(rgb_img_tensor)\n",
    "        img = data_augmentation.array_to_tensor(rgb_img_tensor)\n",
    "        \n",
    "    elif (use_depth and not use_rgb):\n",
    "        img = xyz_img\n",
    "        \n",
    "    elif (use_rgb and use_depth):\n",
    "        rgb_img_tensor = rgb_img.astype(np.float32) # Shape: [H x W x 3]\n",
    "        rgb_img_tensor = data_augmentation.BGR_image(rgb_img_tensor)\n",
    "        rgb_img_tensor = data_augmentation.array_to_tensor(rgb_img_tensor)\n",
    "        img = torch.cat([rgb_img_tensor, xyz_img], dim=0)\n",
    "        \n",
    "    elif use_pretrained:\n",
    "        img = rgb_img.copy()\n",
    "    \n",
    "    \n",
    "    # Run model\n",
    "    predictions = demo.run_on_opencv_image(img, rgb_img) # if show_mask_maps_raw=True, this should be shape: [H x W]\n",
    "\n",
    "    if 'cleargrasp' in dl.dataset.name:\n",
    "        predictions = torch.from_numpy(predictions)[None,...].float()\n",
    "        predictions = cg_dl.filter_labels(predictions, batch['bbox'])\n",
    "        predictions = predictions.numpy()\n",
    "\n",
    "    # Write results to disk\n",
    "    file_path = save_dir + batch['label_abs_path'][0].rsplit('/', 1)[0] + '/'\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    file_name = file_path + batch['label_abs_path'][0].rsplit('/', 1)[1].rsplit('.', 1)[0] + '.png'\n",
    "    util_.imwrite_indexed(file_name, predictions[0].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = save_dir + '/cleargrasp-dataset-test-val/real-val/d435/'\n",
    "files = sorted(os.listdir(temp))\n",
    "\n",
    "i = 30\n",
    "\n",
    "# UOIS-Net-3D prediction\n",
    "filename = os.path.join(temp, files[i])\n",
    "pred_img = util_.imread_indexed(filename)\n",
    "\n",
    "# RGB image\n",
    "rgb_filename = filename.replace(save_dir, '/data/cleargrasp/')\n",
    "rgb_filename = rgb_filename.replace('mask.png', 'transparent-rgb-img.jpg')\n",
    "rgb_img = cv2.cvtColor(cv2.imread(rgb_filename), cv2.COLOR_BGR2RGB)\n",
    "rgb_img = util_.resize_image(rgb_img, (640,368), interpolation='zoom')\n",
    "\n",
    "from scipy.ndimage.measurements import label as connected_components\n",
    "\n",
    "gt_label_filename = filename.replace(save_dir, '/data/cleargrasp/')\n",
    "label_img = util_.imread_indexed(gt_label_filename)\n",
    "if len(label_img.shape) == 3:\n",
    "    label_img = label_img[:, :, 0]\n",
    "\n",
    "label_img, num_components = connected_components(label_img == 255)\n",
    "label_img = util_.resize_image(label_img, (640,368), interpolation='nearest')\n",
    "label_img[label_img > 0] = label_img[label_img > 0] + 1 # so values are in [0, 2, 3, ...] (e.g. no table label)\n",
    "\n",
    "\n",
    "fig = plt.figure(1, figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(rgb_img)\n",
    "plt.title('RGB')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(pred_img)\n",
    "plt.title('Mask RCNN Prediction')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(label_img)\n",
    "plt.title('GT Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:maskrcnn_benchmark]",
   "language": "python",
   "name": "conda-env-maskrcnn_benchmark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
